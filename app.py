ACCESS_CODE = "12345"  # Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ø¯Ø®Ø§Ù„Ù‡

from flask import Flask, render_template_string, request
from playwright.sync_api import sync_playwright, Page

import base64 
#import pytesseract
#import requests
from io import BytesIO
import re
import threading
#import webbrowser

import torch
import time
import cv2
import numpy as np
from sentence_transformers import SentenceTransformer, util
#import pyautogui


#from PIL import ImageChops
# Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ø¨Ù†Ù…ÙˆØ°Ø¬ Ù‚ÙˆÙŠ Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø¹Ù†ÙˆÙŠØ©
transformer_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

from PIL import Image
from datetime import datetime
import os
from rembg import remove

import imagehash

from playwright.sync_api import sync_playwright

import sqlite3
import json
import easyocr
reader = easyocr.Reader(['en','ar'])  # ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ù„Ø¹Ø±Ø¨ÙŠØ©
#result = reader.readtext('path_to_image_or_numpy_array')





#pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

def take_screenshot(page):
    page.wait_for_timeout(500)   # Ù†ØµÙ Ø«Ø§Ù†ÙŠØ© Ø§Ù†ØªØ¸Ø§Ø±
    return page.screenshot(full_page=False)



   
def extract_text_with_boxes(image):
    if image is None:
        print("ğŸš« ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© (image is None)")
        return []

    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØµÙ†Ø§Ø¯ÙŠÙ‚
        results = reader.readtext(image_rgb)
        all_texts = [res[1] for res in results]
        grouped_blocks = [[res[1]] for res in results]


        all_texts = []
        grouped_blocks = []  # â† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¶Ø§ÙØ©: Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ù„ÙˆÙƒØ§Øª Ø§Ù„Ù†ØµÙŠØ©

        current_block_num = -1
        current_block_text = []

        # Ø§Ù„Ù…Ø±ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„ ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        results = reader.readtext(image_rgb)
        all_texts = [res[1] for res in results]
        grouped_blocks = [[res[1]] for res in results]
        print(f"ğŸ“¦ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØªÙ„ Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(all_texts)}")


        print(f"ğŸ“¦ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØªÙ„ Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(all_texts)}")

        # ÙŠÙ…ÙƒÙ†Ùƒ Ù‡Ù†Ø§ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ù…Ø¹Ù‹Ø§ØŒ Ø£Ùˆ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ grouped_blocks Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ
        # return all_texts, grouped_blocks â† ÙÙŠ Ø­Ø§Ù„ Ø£Ø±Ø¯Øª Ø§Ù„Ø¥Ø«Ù†ÙŠÙ† Ù…Ø¹Ù‹Ø§
        return all_texts, grouped_blocks
  # â† Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª ÙÙ‚Ø· all_texts ÙƒÙ…Ø§ Ù‡Ùˆ Ø¸Ø§Ù‡Ø±

    except Exception as e:
        print("â— Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©:", e)
        return []



    



def extract_and_match_posts(all_texts, keywords, similarity_threshold):
    results = []

    # âœ… ØªØ­Ù‚Ù‚ Ø£ÙˆÙ„Ø§Ù‹ Ø£Ù† all_texts ØµØ§Ù„Ø­Ø©
    if not all_texts:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§ (all_texts is empty or None).")
        return []

    

    # ØªØ£ÙƒØ¯ Ø£Ù† ÙƒÙ„ Ø¹Ù†ØµØ± Ù†ØµÙŠ
    try:
        texts = [t.strip() for t in all_texts if isinstance(t, str)]
    except Exception as e:
        print(f"â— Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ: {e}")
        return []

    ##print("....texts....", texts)
    ##input(".......")

    for line in texts:
        if line.strip() == "":
            continue

        #
        #print(f"\n--- Individual Line ---\n{line}\n")

        for kw in keywords:
            sim = compute_similarity(kw, line)

            if sim >= similarity_threshold:
                results.append({
                    "matched_keyword": kw,
                    "similarity": sim,
                    "text": line
                })
                break  # âœ… ØªÙˆÙ‚Ù Ø¨Ø¹Ø¯ Ø£ÙˆÙ„ ØªØ·Ø§Ø¨Ù‚ Ù†Ø§Ø¬Ø­

    return results




def split_into_blocks (text):
    return [block.strip() for block in re.split(r'\n\s*\n', text) if block.strip()]

def is_similar_ai(block, keywords, threshold=0.6):
    best_similarity = 0.0
    best_keyword = None
    for keyword in keywords:
        similarity = compute_similarity(block, keyword)
        if similarity > best_similarity:
            best_similarity = similarity
            best_keyword = keyword
    if best_similarity >= threshold:
        return True, best_similarity, best_keyword
    return False, 0.0, None

def extract_matching_blocks(text, keywords):
    blocks = split_into_blocks(text)
    matched = []
    for block in blocks:
        match, sim, keyword = is_similar_ai(block, keywords)
        if match:
            matched.append((block, sim, keyword))
    return matched


# Ø¥Ø¹Ø¯Ø§Ø¯ Flask
app = Flask(__name__)







def screenshots_are_similar(img_bytes1, img_bytes2, threshold=0.98):
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨Ø§ÙŠØªØ§Øª Ø¥Ù„Ù‰ ØµÙˆØ± PIL
    img1 = Image.open(BytesIO(img_bytes1)).convert("RGB")
    img2 = Image.open(BytesIO(img_bytes2)).convert("RGB")

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ perceptual hash Ù„ÙƒÙ„Ø§ Ø§Ù„ØµÙˆØ±ØªÙŠÙ†
    hash1 = imagehash.phash(img1)
    hash2 = imagehash.phash(img2)

    # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù‡Ø§Ø´Ø§Øª
    diff = hash1 - hash2
    hash_size = len(hash1.hash)**2  # Ø¹Ø¯Ø¯ Ø§Ù„Ø¨ØªØ§Øª ÙÙŠ Ø§Ù„Ù‡Ø§Ø´ (Ø¹Ø§Ø¯Ø© 64)
    
    similarity = 1 - (diff / hash_size)

    #print("ğŸ” pHash similarity:", similarity)
    #input("Ø§Ø¶ØºØ· Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©...")

    return similarity >= threshold



def apply_mask_to_image(image_bytes, mask):
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† bytes Ø¥Ù„Ù‰ NumPy array Ø¨ØµÙŠØºØ© BGR (Ù„Ù€ OpenCV)
    image_stream = BytesIO(image_bytes)
    pil_image = Image.open(image_stream).convert("RGB")
    image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

    # Ø§Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØµÙˆØ±Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„Ù‚Ù†Ø§Ø¹
    if image.shape[:2] != mask.shape:
        mask = cv2.resize(mask, (image.shape[1], image.shape[0]))

    # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ù„ÙÙŠØ© Ø¨ÙŠØ¶Ø§Ø¡
    background = np.ones_like(image, dtype=np.uint8) * 255

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø¥Ù„Ù‰ Ø«Ù„Ø§Ø« Ù‚Ù†ÙˆØ§Øª
    if len(mask.shape) == 2:
        mask_3ch = cv2.merge([mask, mask, mask])
    else:
        mask_3ch = mask

    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ù†Ø§Ø¹
    foreground = cv2.bitwise_and(image, mask_3ch)
    background_masked = cv2.bitwise_and(background, cv2.bitwise_not(mask_3ch))
    final = cv2.add(foreground, background_masked)

    return final



def remove_background_with_rembg(image_bytes):
    # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Bytes ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ RGB
    input_image = Image.open(BytesIO(image_bytes)).convert("RGB")
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
    output_image = remove(input_image)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© NumPy
    output_np = np.array(output_image)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ù†Ø§Ø© Alpha ÙƒÙ‚Ù†Ø§Ø¹ Ø£Ø¨ÙŠØ¶ ÙˆØ£Ø³ÙˆØ¯
    if output_np.shape[2] == 4:
        mask = output_np[:, :, 3]
    else:
        mask = np.ones(output_np.shape[:2], dtype=np.uint8) * 255

    return mask


def is_keyword_match(keyword, text):
    if keyword in text:
        return 1.0  # ØªØ·Ø§Ø¨Ù‚ ØªØ§Ù…
    similarity = compute_similarity(keyword, text)
    return similarity


def compute_similarity(text1, text2, threshold=0.6):
    # ØªÙ‚Ø³ÙŠÙ… text2 Ø¥Ù„Ù‰ Ø¬Ù…Ù„ Ø£Ùˆ Ù…Ù‚Ø§Ø·Ø¹ ÙØ±Ø¹ÙŠØ©
    
    chunks = re.split(r'[.ØŸ!\n]', text2)
    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]

    # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ (text1 + ÙƒÙ„ Ø£Ø¬Ø²Ø§Ø¡ text2)
    texts = [text1] + chunks
    embeddings = transformer_model.encode(texts, convert_to_tensor=True)

    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† text1 ÙˆÙƒÙ„ Ø¬Ø²Ø¡ Ù…Ù† text2
    similarities = util.pytorch_cos_sim(embeddings[0], embeddings[1:])
    max_similarity = similarities.max().item()

    #print("...max similarity with parts of text2......", max_similarity)
    return max_similarity

def save_cookies(context, email):
    cookies = context.cookies()
    if not cookies:  # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ©ØŒ Ù„Ø§ ØªØ­ÙØ¸
        print(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙˆÙƒÙŠØ² Ù„Ù„Ø­ÙØ¸ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…: {email}")
        return
    cookie_json = json.dumps(cookies)
    try:
        conn = sqlite3.connect("/tmp/sessions.db")

        conn.execute("""
            CREATE TABLE IF NOT EXISTS cookies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT UNIQUE,
                cookie_json TEXT
            )
        """)
        conn.execute("REPLACE INTO cookies (user_id, cookie_json) VALUES (?, ?)", (email, cookie_json))
        conn.commit()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {email}: {e}")
    finally:
        conn.close()


def load_cookies(context, email):
    try:
        conn = sqlite3.connect("/tmp/sessions.db")

        conn.execute("""
            CREATE TABLE IF NOT EXISTS cookies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT UNIQUE,
                cookie_json TEXT
            )
        """)
        cursor = conn.execute("SELECT cookie_json FROM cookies WHERE user_id = ?", (email,))
        row = cursor.fetchone()
        if row:
            cookies = json.loads(row[0])
            context.add_cookies(cookies)
        else:
            print(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙˆÙƒÙŠØ² Ù…Ø­ÙÙˆØ¸Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…: {email}")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {email}: {e}")
    finally:
        conn.close()


def get_dom_snapshot(page):
    try:
        return page.content()
    except Exception as e:
        print("âŒ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© DOM:", e)
        return ""
    


def preprocess_image_for_ocr(image):
    # 1. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø±Ù…Ø§Ø¯ÙŠØ©
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # 2. Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… CLAHE
    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8,8))
    enhanced = clahe.apply(gray)

    # 3. ØªØ·Ø¨ÙŠÙ‚ Threshold Ø°ÙƒÙŠ (Adaptive Threshold) Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø³ÙˆØ¯ ÙˆØ§Ù„Ø®Ù„ÙÙŠØ© Ø¥Ù„Ù‰ Ø£Ø¨ÙŠØ¶
    thresh = cv2.adaptiveThreshold(
        enhanced, 255, 
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
        cv2.THRESH_BINARY,21 , 10
    )

    # 4. Ø¹
    processed = thresh

    return processed


import asyncio


def expand_all_facebook_posts(page):
    try:
        # 1. Ø§Ù„ØªÙ‚Ø§Ø· Ø³ÙƒØ±ÙŠÙ† Ø´ÙˆØª Ù…Ù† Ø§Ù„ØµÙØ­Ø© (Full page screenshot ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø§ØªÙØŒ Ù†Ø£Ø®Ø° Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© ÙÙ‚Ø·)
        #input("i will screen shot")
        screenshot_bytes = take_screenshot(page)


        image_stream = BytesIO(screenshot_bytes)
        pil_image = Image.open(image_stream).convert("RGB")

        # 2. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ numpy array Ù„Ù€ OpenCV
        image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

        # Ø«Ù… Ø·Ø¨Ù‚ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† (Ù…Ø«Ù„Ø§Ù‹ Ø¯Ø§Ù„Ø© preprocess_image_for_ocr Ø§Ù„ØªÙŠ Ø°ÙƒØ±ØªÙ‡Ø§ Ù„Ùƒ)
        image = preprocess_image_for_ocr(image)

        # 3. Ø§Ø³ØªØ®Ø¯Ù… pytesseract Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        #data = pytesseract.image_to_data(image, lang='eng+ara', output_type=pytesseract.Output.DICT)
        
        # 3. Ø§Ø³ØªØ®Ø¯Ù… EasyOCR Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ…ÙˆØ§Ù‚Ø¹Ù‡Ø§
        results = reader.readtext(image)  # reader Ù‡Ùˆ EasyOCR reader
        targets = []

        for bbox, text, conf in results:
            text_lower = text.lower().strip()
            if text_lower in ["see more", "Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²ÙŠØ¯"]:
                (x0, y0), (x1, y1), (x2, y2), (x3, y3) = bbox
                center_x = int((x0 + x1 + x2 + x3) / 4)
                center_y = int((y0 + y1 + y2 + y3) / 4)
                targets.append((center_x, center_y))
                print(f"âœ… Found target '{text}' at: ({center_x}, {center_y})")



        print("targets........")
        # 5. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø£Ù‡Ø¯Ø§ÙØŒ Ø§Ø±Ø¬Ø¹ Ø¨Ø¹Ø¯ ØªØ£Ø®ÙŠØ±
        if not targets:
            time.sleep(2)
            return False

        # 6. Ø¥Ø°Ø§ ÙˆØ¬Ø¯ØªØŒ Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ ÙƒÙ„ Ù‡Ø¯Ù Ù…Ø¹ ÙØ§ØµÙ„ 2 Ø«Ø§Ù†ÙŠØ©
        # Ù‚Ø¨Ù„ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„ØªÙŠ ØªÙ†Ù‚Ø± ÙÙŠÙ‡Ø§ Ø£Ø¶Ù Ù‡Ø°Ù‡ Ø§Ù„Ø³Ø·ÙˆØ± Ù„ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØµÙˆØ±Ø© ÙˆÙ†Ø§ÙØ°Ø© Ø§Ù„Ù…ØªØµÙØ­:
        img_width, img_height = pil_image.size
        viewport = page.viewport_size
        vp_width, vp_height = viewport['width'], viewport['height']

        

        for (x_img, y_img) in targets:
                device_scale_factor = page.evaluate("window.devicePixelRatio")

                x_page = int(x_img * vp_width / img_width / device_scale_factor) + 3
                y_page = int(y_img * vp_height / img_height / device_scale_factor) + 3

                print(f"Clicking at: ({x_page}, {y_page})")
                try:
                    print(f"ğŸ‘† Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¶ØºØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø§ÙˆØ³ ÙÙŠ ({x_page}, {y_page})")
                    page.mouse.move(x_page, y_page)
                    time.sleep(0.1)
                    page.mouse.click(x_page, y_page)
                    time.sleep(2)
                except Exception as e:
                    print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ù†Ù‚Ø± Ø¨Ø§Ù„Ù…Ø§ÙˆØ³ â€” ØªØ¬Ø±Ø¨Ø© JavaScript: {e}")
                    page.evaluate(f"""
                        () => {{
                            const el = document.elementFromPoint({x_page}, {y_page});
                            if (el) el.click();
                        }}
                    """)

                print("Clicked")



        # 7. ÙØ§ØµÙ„ 2 Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ø±Ø¬ÙˆØ¹
        time.sleep(2)
        return True

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ expand_all_facebook_posts: {e}")
        return False



def smart_scroll_mobile(page):
    #page.evaluate("window.scrollBy(0, window.innerHeight * 2);")
    #page.wait_for_timeout(3000)
    page.evaluate("""
        const el = document.querySelector('[role="feed"]') || document.querySelector('div[data-pagelet]');
        if (el) {
            el.scrollTop -= 200;  // Scroll up Ù‚Ù„ÙŠÙ„Ø§Ù‹
            el.scrollTop += 200;  // Scroll down Ù‚Ù„ÙŠÙ„Ø§Ù‹

        } else {
            window.scrollBy(0, -200);
            window.scrollBy(0, 200);
        }
    """);
    page.wait_for_timeout(300);

    page.evaluate("window.scrollBy(0, window.innerHeight * 1);")
    page.wait_for_timeout(500);
    return 


import random
import time

def scroll_up_and_return_smoothly(page):
    import random

    # 1. Ø­ÙØ¸ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ
    current_scroll = page.evaluate("() => window.scrollY")
    print(f"ğŸ“Œ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­ÙÙˆØ¸: {current_scroll}")
    if current_scroll <= 0:
       print("âš ï¸ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø­ÙÙˆØ¸ Ù‡Ùˆ Ø£Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø©. Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠ Ø§Ù„Ø¹ÙˆØ¯Ø©.")
       return


    # 2. Ø§Ù„ØµØ¹ÙˆØ¯ Ù„Ø£Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ù€ 20 Ø®Ø·ÙˆØ© ØªØ¯Ø±ÙŠØ¬ÙŠØ©
    scroll_steps = random.randint(10, 20)
    for i in range(scroll_steps):
        page.evaluate("window.scrollBy(0, -window.innerHeight / 2);")
        wait = random.randint(100, 300)
        page.wait_for_timeout(wait)

        if i % 5 == 0:
            pause = random.randint(500, 1500)
            print(f"â¸ï¸ ØªÙˆÙ‚Ù Ù‚ØµÙŠØ±: {pause}ms")
            page.wait_for_timeout(pause)

    print("â¬†ï¸ ØªÙ… Ø§Ù„ØµØ¹ÙˆØ¯ Ù„Ø£Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø©.")
    scroll_fraction = random.uniform(0.5, 1.2)  # Ù…Ù† Ù†ØµÙ Ø¥Ù„Ù‰ 1.2 Ù…Ù† Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ø´Ø§Ø´Ø©
    page.evaluate(f"window.scrollBy(0, window.innerHeight * {scroll_fraction});")
    page.wait_for_timeout(random.randint(300, 700))

    page.evaluate(f"window.scrollBy(0, -window.innerHeight * {scroll_fraction});")

    
    
    # ØªÙˆÙ‚Ù Ø¥Ø¶Ø§ÙÙŠ ÙŠØ´Ø¨Ù‡ Ø§Ù„ØªØ£Ù…Ù„ Ø£Ùˆ Ø§Ù„ØªÙÙƒÙŠØ±
    if random.random() < 0.3:
        pause = random.randint(800, 1500)
        print(f"ğŸ¤” ØªÙˆÙ‚Ù Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„ØªØ£Ù…Ù„ Ù„Ù…Ø¯Ø© {pause}ms...")
        page.wait_for_timeout(pause)


    # 3. ØªØµØ±ÙØ§Øª Ø¨Ø´Ø±ÙŠØ© Ø®ÙÙŠÙØ© Ø¨Ø¯Ù„ Ø§Ù„Ù†Ù‚Ø±
    for i in range(3):
        x = random.randint(50, 300)
        y = random.randint(100, 600)

        # ØªØ­Ø±ÙŠÙƒ Ø§Ù„Ù…Ø§ÙˆØ³ Ø¨Ø´ÙƒÙ„ Ø®ÙÙŠÙ
        for step in range(5):
            move_x = x + random.randint(-5, 5)
            move_y = y + random.randint(-5, 5)
            page.mouse.move(move_x, move_y)
            page.wait_for_timeout(random.randint(30, 60))

        # Ù…Ø­Ø§ÙƒØ§Ø© Ù‚Ø±Ø§Ø¡Ø© Ø£Ùˆ ØªÙ…Ø¹Ù†
        page.wait_for_timeout(random.randint(400, 800))

        # ØªÙ…Ø±ÙŠØ± Ø¨Ø³ÙŠØ· Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¬Ø¯Ø§Ù‹ (Ø´Ø¨ÙŠÙ‡ Ø¨Ù„Ù…Ø³ Ø¥ØµØ¨Ø¹)
        delta_y = random.randint(-50, 50)
        page.evaluate(f"window.scrollBy(0, {delta_y});")
        print(f"ğŸ¤ ØªÙ…Ø±ÙŠØ± Ø¨Ø´Ø±ÙŠ Ø®ÙÙŠÙ Ø¨Ù…Ù‚Ø¯Ø§Ø± {delta_y}px")
        page.wait_for_timeout(random.randint(300, 700))
        page.evaluate(f"window.scrollBy(0, {-delta_y});")

    # 4. ØªÙ…Ø±ÙŠØ± Ø°ÙƒÙŠ Ù„Ù„Ø£Ø³ÙÙ„
    smart_scroll_mobile(page)
    page.evaluate("window.scrollBy(0, -window.innerHeight);")


    # 5. Ø§Ù„Ø¹ÙˆØ¯Ø© ØªØ¯Ø±ÙŠØ¬ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ
    steps = random.randint(10, 20)
    step_size = current_scroll // steps

    for i in range(steps):
        target_scroll = (i + 1) * step_size
        page.evaluate(f"window.scrollTo(0, {target_scroll});")
        wait = random.choice([90, 120, 150, 180])

        page.wait_for_timeout(wait)

    print("â¬‡ï¸ ØªÙ… Ø§Ù„Ø±Ø¬ÙˆØ¹ ØªØ¯Ø±ÙŠØ¬ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£ØµÙ„ÙŠ.")




def analyze_posts_via_screenshot_ai(page: Page, keywords, max_posts, max_scrolls, similarity_threshold):

    seen_texts = set()
    scroll_count = 0
    analyzed_count = 0
    posts_data = []
    last_screenshot = None
    last_dom_snapshot = ""
    
   
   
    smart_scroll_mobile(page)
    smart_scroll_mobile(page)
    smart_scroll_mobile(page)
    smart_scroll_mobile(page)

    
    while analyzed_count < max_posts and scroll_count < max_scrolls:
        
        
        
        current_dom_snapshot = get_dom_snapshot(page)

        #if current_dom_snapshot != last_dom_snapshot:
        #    print("ğŸ“„ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯")
        expanded = expand_all_facebook_posts(page)
            #smart_scroll_mobile(page)
            #page.wait_for_timeout(1500)
            
        #    last_dom_snapshot = current_dom_snapshot
       # else:
        #    print("ğŸŸ¡ Ù†ÙØ³ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø³Ø§Ø¨Ù‚ â€” Ù„Ù† Ù†Ù†ÙØ° expand")



        # Ø«Ù… Ù†Ø£Ø®Ø° screenshot ÙƒØ§Ù„Ù…Ø¹ØªØ§Ø¯
        screenshot_bytes = page.screenshot(full_page=False)





      
        if last_screenshot and screenshots_are_similar(screenshot_bytes, last_screenshot):
            print("[Scroll Detection] Screenshot is similar. Skipping...")
            scroll_count += 1
            if 2 <= scroll_count <= 4:

                # Ø­ÙØ¸ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ
                

                # Ø§Ù„ØµØ¹ÙˆØ¯ Ù„Ø£Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø©
                
                scroll_up_and_return_smoothly(page)
                #scroll_up_and_return_smoothly(page)
                            
            continue
        else:
            #print("[Scroll Detection] First screenshot.")
            #input("111111111")
            
            scroll_count = 0
            last_screenshot = screenshot_bytes
            print(scroll_count)
        masked_image = None  # ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ØªØºÙŠØ± Ù‚Ø¨Ù„ try

        try:
            mask = remove_background_with_rembg(screenshot_bytes)
            masked_image = apply_mask_to_image(screenshot_bytes, mask)
        except Exception as e:
            print("Mask extraction failed:", e)
            try:
                image_stream = BytesIO(screenshot_bytes)
                pil_image = Image.open(image_stream).convert("RGB")
                masked_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            except Exception as e2:
                print("Fallback failed:", e2)
                masked_image = None

        
        if masked_image is not None:
            _, buffer = cv2.imencode('.png', masked_image)
            image_base64 = base64.b64encode(buffer).decode("utf-8")
        else:
            image_base64 = ""


        all_texts, grouped_blocks = extract_text_with_boxes(masked_image)

        #print("...all text...",all_texts)
        #input(".....")

        for i, full_text in enumerate(all_texts):

            #print("...full text...",full_text)
            #input("......")
            for keyword in keywords:
                similarity = compute_similarity(full_text.strip(), keyword.strip())
                #print("...similarity....",similarity)
                #input("......")
                if similarity >= similarity_threshold:

                    
                    
                    matched_block = grouped_blocks[i]
                    #print("âœ… Bloc Ù…Ø·Ø§Ø¨Ù‚:", ' '.join(matched_block))
                    #input(".....")

                   # ØªØ±Ù…ÙŠØ² Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ base64
                    image_base64 = base64.b64encode(screenshot_bytes).decode("utf-8")

                    posts_data.append({
                        "text": ' '.join(matched_block),
                        "matched_keyword": keyword,
                        "similarity": f"{similarity:.2f}",
                        "image_base64": image_base64,
                        "page_url": page.url
                    })

                    break


        analyzed_count += 1
        if analyzed_count < max_posts:
            smart_scroll_mobile(page)
            #page.wait_for_timeout(1500)





    posts_data.sort(key=lambda x: x.get("similarity", 0), reverse=True)


    return posts_data





def run_facebook_scraper(keywords, facebook_pages, max_posts, similarity_threshold, email):
    max_scrolls = 5

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=["--no-sandbox", "--disable-setuid-sandbox"]
        )



        context = browser.new_context(
            user_agent="Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36"
        )

        load_cookies(context, email)

        page = context.new_page()
        all_results = []

        for fb_page in facebook_pages:
            print(f"\nğŸ”— ÙØªØ­ Ø§Ù„Ø±Ø§Ø¨Ø·: {fb_page}")
            fb_page = fb_page.replace("www.facebook.com", "m.facebook.com")

            try:
                page.goto(fb_page, timeout=120000)
                print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­")
            except Exception as e:
                print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©:", e)
                continue

            print("ğŸ”’ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„...")
            try:
                search_input = page.locator("input[name='query']")
                print("â³ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø´Ø±ÙŠØ· Ø§Ù„Ø¨Ø­Ø«...")
                search_input.wait_for(state="visible", timeout=5000)
                print("âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø´Ø±ÙŠØ· Ø§Ù„Ø¨Ø­Ø«")
                search_input.fill(keywords[0])
                search_input.press("Enter")
                page.wait_for_timeout(5000)
            except Exception as e:
                print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø´Ø±ÙŠØ· Ø§Ù„Ø¨Ø­Ø« â€” Ù†ØªØ§Ø¨Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø±Ø© (Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø¬Ù…ÙˆØ¹Ø©):")
                print(f"ğŸ” Ø§Ù„Ø®Ø·Ø£: {e}")

            smart_scroll_mobile(page)
            smart_scroll_mobile(page)
            page.keyboard.press("PageDown")
            page.keyboard.press("PageDown")



            print("âœ¨ Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª...")
            try:
                expanded = expand_all_facebook_posts(page)
                if not expanded:
                    print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø²Ø± 'Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²ÙŠØ¯' â€” Ù†ØªØ§Ø¨Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„.")
            except Exception as e:
                print("âš ï¸ ÙØ´Ù„ ÙÙŠ Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª:", e)


            print("ğŸ¯ Ù…ØªØ§Ø¨Ø¹Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª...")
            try:
                posts_data = analyze_posts_via_screenshot_ai(
                    page, keywords, max_posts, max_scrolls, similarity_threshold
                )
                print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {len(posts_data)}")
                all_results.extend(posts_data)

            except Exception as e:
                if 'posts_data' in locals() and posts_data:
                    print(f"âš ï¸ Ø³ÙŠØªÙ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ {len(posts_data)} Ù…Ù†Ø´ÙˆØ±Ù‹Ø§ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡ Ù‚Ø¨Ù„ Ø§Ù„Ø®Ø·Ø£.")
                    all_results.extend(posts_data)
                else:
                    print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù…Ù†Ø´ÙˆØ±Ø§Øª Ù‚Ø¨Ù„ Ø­Ø¯ÙˆØ« Ø§Ù„Ø®Ø·Ø£.")

            save_cookies(context, email)

        browser.close()
        return all_results

        




def cleanup_static_images():
    folder = "static"
    for filename in os.listdir(folder):
        if filename.startswith("match_") and filename.endswith(".png"):
            os.remove(os.path.join(folder, filename))


# ÙˆØ§Ø¬Ù‡Ø© HTML Ø¨Ø³ÙŠØ·Ø©
HTML_TEMPLATE = '''
<!DOCTYPE html>
<html lang="en">
<head>

<script>
function openImage(src) {
  const w = window.open("", "_blank", "width=900,height=600,resizable=yes");
  w.document.write(`
    <html><head><title>Enlarged Image</title></head>
    <body style="margin:0;display:flex;justify-content:center;align-items:center;background:#000;">
      <img src="${src}" style="max-width:100%;max-height:100%;">
    </body></html>
  `);
}
</script>

    <meta charset="UTF-8">
    <title>Facebook Post Analyzer</title>
</head>
<body>
    <h2>Analyze Facebook Posts   /  malaz janbeih-malazjanbeih@gmail.com-+96170647081  </h2>
    <form method="POST">
        <label>User Email or ID:</label><br>
        <input type="text" name="email" required><br><br>

        <label>Facebook Links (one per line):</label><br>
        <textarea name="links" rows="10" cols="60" style="border:1px solid #ccc; padding:5px; line-height:1.5; font-family: monospace; white-space: pre-wrap;"></textarea><br><br>
        <label>Keywords (separated by -):</label><br>
        <input type="text" name="keywords" size="60"><br><br>
        <label>Max Posts per Link:</label><br>
        <input type="number" name="max_posts" min="1" placeholder="Enter max posts"><br><br>
        <label>Similarity Threshold (default 0.8):</label><br>
        <input type="number" name="similarity" step="0.1" min="0.5" max="1.0" value="0.8"><br><br>
        <label>Enter Access Code:</label><br>
        <input type="text" name="access_code" required><br><br>

        <input type="submit" value="Analyze">
    </form>

   {% if message %}
    <p style="color:red;"><strong>{{ message }}</strong></p>
    {% endif %}

    {% for r in results %}
        <li>
            <strong>Matched Keyword:</strong> {{ r.matched_keyword }}<br>
            <strong>Text:</strong> <pre>{{ r.text }}</pre><br>
            <img src="data:image/png;base64,{{ r.image_base64 }}" width="300"
                style="cursor: zoom-in; border: 1px solid #ccc; border-radius: 5px;"
                onclick="openImage(this.src)">
        </li><hr>
    {% endfor %}

</body>
</html>
'''



@app.route("/", methods=["GET", "POST"])
def home():
    message = ""
    results = []

    if request.method == "POST":
        access_code = request.form.get("access_code", "").strip()
        if access_code != ACCESS_CODE:
            message = "âŒ Incorrect Access Code."
            return render_template_string(HTML_TEMPLATE, message=message)

        links = request.form["links"].strip().splitlines()
        keywords = request.form["keywords"].strip().split("-")
        max_posts = int(request.form["max_posts"])
        similarity_input = request.form.get("similarity", "0.7")
        try:
            similarity_threshold = float(similarity_input)
        except:
            similarity_threshold = 0.7

        email = request.form.get("email", "").strip()

        results = run_facebook_scraper(keywords, links, max_posts, similarity_threshold, email)

        if not results:
            message = "â— No matched keyword found."
        return render_template_string(HTML_TEMPLATE, results=results, message=message)

    return render_template_string(HTML_TEMPLATE)







if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port)

